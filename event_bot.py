# -*- coding: utf-8 -*-
"""
event_bot.py
Walker+ ã¨ Tokyo Art Beat ã‚’çµ±åˆã—ã¦å–å¾—ã—ã€
Google Sheets ã«é€ä¿¡æ¸ˆã¿ã‚’ä¿å­˜ã€LINEå…¬å¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«é…ä¿¡ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚

Install:
  pip install requests beautifulsoup4 gspread google-auth
Env (GitHub Secrets):
  - GOOGLE_CREDENTIALS  (service account json string)
  - SPREADSHEET_ID
  - LINE_ACCESS_TOKEN
"""
import os
import re
import json
import time
import random
import logging
import urllib.parse
from datetime import datetime

import requests
from bs4 import BeautifulSoup
import gspread
from google.oauth2.service_account import Credentials

# ============= ãƒ­ã‚°è¨­å®š =============
logging.basicConfig(level=logging.INFO, format="%(asctime)s\t%(levelname)s\t%(message)s")

# ============= Google Sheets è¨­å®š =============
SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]
SPREADSHEET_ID = os.getenv("SPREADSHEET_ID")
SHEET_NAME = "sent_events"

def init_sheet():
    if not SPREADSHEET_ID:
        raise RuntimeError("SPREADSHEET_ID not set")
    creds_json = os.environ["GOOGLE_CREDENTIALS"]
    creds_dict = json.loads(creds_json)
    creds = Credentials.from_service_account_info(creds_dict, scopes=SCOPES)
    gc = gspread.authorize(creds)
    sh = gc.open_by_key(SPREADSHEET_ID)
    try:
        sheet = sh.worksheet(SHEET_NAME)
    except gspread.exceptions.WorksheetNotFound:
        sheet = sh.add_worksheet(title=SHEET_NAME, rows=1000, cols=10)
        sheet.append_row(["é€ä¿¡æ—¥", "ã‚¤ãƒ™ãƒ³ãƒˆå", "é–‹å‚¬æœŸé–“", "URL", "ä¼šå ´"])
    return sheet

# ============= å±¥æ­´åˆ¤å®šãƒ»ä¿å­˜ =============
def already_sent(event_url, sheet):
    if not event_url:
        return False
    try:
        records = sheet.col_values(4)  # URL åˆ—
        return event_url in records
    except Exception as e:
        logging.warning("Could not read sheet column: %s", e)
        return False

def save_event(sheet, event):
    """
    event: dict with keys possibly 'name','start','end','official_url','url','venue'
    Saves a row: [é€ä¿¡æ—¥, ã‚¤ãƒ™ãƒ³ãƒˆå, é–‹å‚¬æœŸé–“, URL, ä¼šå ´]
    """
    today = datetime.now().strftime("%Y-%m-%d")
    name = event.get("name", "")
    # normalize period
    start = event.get("start") or event.get("startDate") or ""
    end = event.get("end") or event.get("endDate") or ""
    def norm(d):
        if not d: return ""
        if isinstance(d, str) and re.match(r"\d{4}-\d{2}-\d{2}", d):
            return d[:10]
        return d
    if start and end:
        period = f"{norm(start)} ï½ {norm(end)}"
    elif start:
        period = norm(start)
    else:
        period = ""
    url = event.get("official_url") or event.get("url") or ""
    venue = event.get("venue") or ""
    try:
        sheet.append_row([today, name, period, url, venue])
    except Exception as e:
        logging.warning("Failed to append row: %s", e)

# ============= LINEé€ä¿¡ =============
LINE_ACCESS_TOKEN = os.getenv("LINE_ACCESS_TOKEN")
HEADERS = {
    "Content-Type": "application/json; charset=UTF-8",
    "Authorization": f"Bearer {LINE_ACCESS_TOKEN}" if LINE_ACCESS_TOKEN else ""
}

def send_line_broadcast(message: str):
    if not LINE_ACCESS_TOKEN:
        logging.warning("LINE_ACCESS_TOKEN not set: skipping broadcast")
        return
    payload = {"messages": [{"type": "text", "text": message}]}
    try:
        resp = requests.post("https://api.line.me/v2/bot/message/broadcast",
                             headers=HEADERS, data=json.dumps(payload).encode("utf-8"), timeout=15)
        logging.info("LINE send: %s %s", resp.status_code, resp.text)
    except Exception as e:
        logging.error("LINE send error: %s", e)

# ============= Walker+ å–å¾— (JSON-LD) =============
def fetch_walkerplus_events(base_url: str, max_pages: int = 2):
    events = []
    headers = {"User-Agent": "Mozilla/5.0"}
    for page in range(1, max_pages + 1):
        url = base_url if page == 1 else f"{base_url}{page}.html"
        logging.info("Walker+ fetch: %s", url)
        try:
            res = requests.get(url, timeout=15, headers=headers)
            res.raise_for_status()
            soup = BeautifulSoup(res.text, "html.parser")
            for tag in soup.find_all("script", {"type": "application/ld+json"}):
                try:
                    data = json.loads(tag.string)
                    if isinstance(data, list):
                        for ev in data:
                            if ev.get("@type") == "Event":
                                events.append(ev)
                    elif isinstance(data, dict) and data.get("@type") == "Event":
                        events.append(data)
                except Exception:
                    continue
        except Exception as e:
            logging.warning("Walker+ page fetch failed: %s", e)
    logging.info("Walker+ total events found: %d", len(events))
    return events

# ============= TokyoArtBeat å–å¾— (requests + BeautifulSoup äºŒæ®µéš) =============
UA = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36"
EXCLUDE_DOMAINS = {
    "art.nikkei.com", "doubleclick.net", "adservice.google.com",
    "instagram.com", "twitter.com", "facebook.com", "x.com", "youtube.com",
    "lin.ee", "mailchi.mp"
}
sess = requests.Session()
sess.headers.update({"User-Agent": UA})

def get_soup(url, timeout=20):
    r = sess.get(url, timeout=timeout)
    r.raise_for_status()
    return BeautifulSoup(r.text, "html.parser")

def uniq_preserve(seq):
    seen = set(); out=[]
    for s in seq:
        if s not in seen:
            seen.add(s); out.append(s)
    return out

def extract_official_url_from_soup(soup, venue_hint=None):
    # å„ªå…ˆ: "å±•è¦§ä¼šURL" ãªã©ãƒ©ãƒ™ãƒ«ã‹ã‚‰æ¢ã™
    label_patterns = [r"å±•è¦§ä¼šURL", r"å±•è¦§ä¼šã‚µã‚¤ãƒˆ", r"å…¬å¼ã‚µã‚¤ãƒˆ", r"å…¬å¼ï¼¨?ï¼°", r"å…¬å¼ãƒšãƒ¼ã‚¸", r"Official site", r"Official website", r"Website"]
    for pat in label_patterns:
        node = soup.find(string=re.compile(pat))
        if node:
            parent = node.parent
            a = parent.find("a", href=True)
            if a and a["href"].startswith("http"):
                return a["href"].strip()
            # è¿‘å‚ã‚‚ãƒã‚§ãƒƒã‚¯
            sib = parent.next_sibling; tries=0
            while sib and tries < 6:
                if getattr(sib, "find", None):
                    a2 = sib.find("a", href=True)
                    if a2 and a2["href"].startswith("http"):
                        return a2["href"].strip()
                sib = getattr(sib, "next_sibling", None); tries += 1
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¤–éƒ¨ãƒªãƒ³ã‚¯ã®ã†ã¡é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’é¿ã‘ã¦æœ€åˆã®ã‚‚ã®
    anchors = soup.find_all("a", href=True)
    candidates = []
    for a in anchors:
        href = a["href"].strip()
        if not href.startswith("http"): continue
        netloc = urllib.parse.urlparse(href).netloc.lower()
        if "tokyoartbeat.com" in netloc: continue
        bad = any(bad in netloc for bad in EXCLUDE_DOMAINS)
        candidates.append((bad, href))
    for bad, href in candidates:
        if not bad:
            if venue_hint and venue_hint.lower() in href.lower():
                return href
            return href
    if candidates:
        return candidates[0][1]
    return ""

def extract_venue_from_soup(soup):
    node = soup.find(string=re.compile(r"ä¼šå ´"))
    if node:
        parent = node.parent
        a = parent.find("a")
        if a and a.get_text(strip=True): return a.get_text(strip=True)
        txt = parent.get_text(" ", strip=True)
        m = re.search(r"ä¼šå ´[:ï¼š\s]*(.+?)(?:ä½æ‰€|ã€’|æ™‚é–“|$)", txt)
        if m: return m.group(1).strip()
    venue_tag = soup.select_one(".venue, .location, a[href*='/venue/'], a[href*='/venues/']")
    if venue_tag: return venue_tag.get_text(strip=True)
    return ""

def extract_date_range_from_text(text):
    if not text: return "", ""
    p1 = re.search(r"(\d{4})å¹´\D*(\d{1,2})æœˆ\D*(\d{1,2})æ—¥.*?ã€œ.*?(\d{4})å¹´\D*(\d{1,2})æœˆ\D*(\d{1,2})æ—¥", text)
    if p1:
        y1,m1,d1,y2,m2,d2 = p1.groups()
        return f"{int(y1):04d}-{int(m1):02d}-{int(d1):02d}", f"{int(y2):04d}-{int(m2):02d}-{int(d2):02d}"
    p2 = re.search(r"(\d{4})å¹´\D*(\d{1,2})æœˆ\D*(\d{1,2})æ—¥.*?ã€œ.*?(\d{1,2})æœˆ\D*(\d{1,2})æ—¥", text)
    if p2:
        y1,m1,d1,m2,d2 = p2.groups()
        return f"{int(y1):04d}-{int(m1):02d}-{int(d1):02d}", f"{int(y1):04d}-{int(m2):02d}-{int(d2):02d}"
    p3 = re.search(r"(\d{4})å¹´\D*(\d{1,2})æœˆ\D*(\d{1,2})æ—¥", text)
    if p3:
        y,m,d = p3.groups(); s = f"{int(y):04d}-{int(m):02d}-{int(d):02d}"; return s, s
    iso = re.search(r"(\d{4}-\d{2}-\d{2})", text)
    if iso: return iso.group(1), iso.group(1)
    return "", ""

def fetch_tokyoartbeat_officials(list_url, max_items=10, politeness=(0.5,1.2)):
    logging.info("Fetching TokyoArtBeat list: %s", list_url)
    list_soup = get_soup(list_url)
    anchors = list_soup.find_all("a", href=re.compile(r"^/events/[-/]"))
    paths = [a["href"] for a in anchors if a.get("href")]
    paths = [p for p in paths if "/events/top" not in p and "/events/condId" not in p]
    paths = uniq_preserve(paths)

    results = []
    for rel in paths[:max_items]:
        detail_url = urllib.parse.urljoin("https://www.tokyoartbeat.com", rel)
        try:
            dsoup = get_soup(detail_url)
        except Exception as e:
            logging.warning("Failed fetch detail %s : %s", detail_url, e)
            continue

        h1 = dsoup.find("h1")
        name = h1.get_text(strip=True) if h1 else (dsoup.title.string.strip() if dsoup.title else "")

        # schedule extraction
        sched_node = dsoup.find(string=re.compile(r"ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«|é–‹å‚¬æœŸé–“|ä¼šæœŸ"))
        if sched_node:
            schedule_text = sched_node.parent.get_text(" ", strip=True)
        else:
            body = dsoup.get_text(" ", strip=True)
            m = re.search(r".{0,160}\d{4}å¹´.*?ã€œ.*?\d{1,4}æ—¥.{0,40}", body)
            schedule_text = m.group(0) if m else body[:250]
        start, end = extract_date_range_from_text(schedule_text)

        venue = extract_venue_from_soup(dsoup)
        official = extract_official_url_from_soup(dsoup, venue_hint=venue)
        if not official:
            # fallback: look for external link near venue
            node = dsoup.find(string=re.compile(r"ä¼šå ´"))
            if node:
                p = node.parent; a = p.find("a", href=True)
                if a and a["href"].startswith("http"): official = a["href"].strip()

        results.append({
            "name": name,
            "start": start,
            "end": end,
            "venue": venue,
            "official_url": official
        })
        time.sleep(random.uniform(*politeness))
    logging.info("TokyoArtBeat events fetched: %d", len(results))
    return results

# ============= ä¼šå ´ -> éƒ½çœŒ ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆãƒ™ã‚¹ãƒˆã‚¨ãƒ•ã‚©ãƒ¼ãƒˆï¼‰ =============
PREF_KEYWORD_MAP = {
    "æ±äº¬": ["æ±äº¬éƒ½","æ±äº¬","ä¸Šé‡","å…­æœ¬æœ¨","éŠ€åº§","æ–°å®¿","æµ…è‰","æ¸‹è°·","æ± è¢‹","ãŠå°å ´","å“å·","ä¸¸ã®å†…","æ—¥æ¯”è°·","æœ‰æ¥½ç”º","å›½ç«‹æ–°ç¾è¡“é¤¨","æ±äº¬éƒ½ç¾è¡“é¤¨","å›½ç«‹è¥¿æ´‹ç¾è¡“é¤¨","æ£®ç¾è¡“é¤¨","å…­æœ¬æœ¨ãƒŸãƒ¥ãƒ¼ã‚¸ã‚¢ãƒ ","MMM","ãƒ¡ã‚¾ãƒ³ãƒ»ãƒ‡ãƒ»ãƒŸãƒ¥ã‚¼","ã‚¹ã‚«ã‚¤ãƒ„ãƒªãƒ¼"],
    "ç¥å¥ˆå·": ["æ¨ªæµœ","å·å´","éŒå€‰","è—¤æ²¢","ç›¸æ¨¡åŸ","ç®±æ ¹","æ–°æ¨ªæµœ","ã¿ãªã¨ã¿ã‚‰ã„","æ¨ªæµœç¾è¡“é¤¨","æ¨ªæµœå¸‚","æ¨ªæµœæ¸¯"],
    "åƒè‘‰": ["å¹•å¼µ","åƒè‘‰","æˆç”°","èˆ¹æ©‹","å¸‚å·","æµ¦å®‰","å¹•å¼µãƒ¡ãƒƒã‚»","å¹•å¼µæ–°éƒ½å¿ƒ","æˆ¿ç·","æµ·æµœå¹•å¼µ"],
    "åŸ¼ç‰": ["åŸ¼ç‰","å¤§å®®","æ‰€æ²¢","å·è¶Š","è¶Šè°·","ç§©çˆ¶","ç†Šè°·","ç‹­å±±","å…¥é–“","é£¯èƒ½","æ‰€æ²¢","ã•ã„ãŸã¾"]
}

def map_venue_to_prefecture(venue: str, official_url: str = ""):
    if venue:
        v = venue.lower()
        for pref, keys in PREF_KEYWORD_MAP.items():
            for k in keys:
                if k.lower() in v:
                    return pref
    if official_url:
        u = official_url.lower()
        if any(x in u for x in ["yokohama","kanagawa","kawasaki","sagamihara","yokosuka"]):
            return "ç¥å¥ˆå·"
        if any(x in u for x in ["makuhari","chiba","narita","funabashi","urayasu","sakura"]):
            return "åƒè‘‰"
        if any(x in u for x in ["saitama","omiya","kawagoe","tokorozawa","hanno","hanno"]):
            return "åŸ¼ç‰"
        if any(x in u for x in ["tokyo","roppongi","ginza","asakusa","ueno","shibuya","shinjuku","tocho"]):
            return "æ±äº¬"
    # try explicit kanji
    if "ç¥å¥ˆå·" in (venue or ""): return "ç¥å¥ˆå·"
    if "åƒè‘‰" in (venue or ""): return "åƒè‘‰"
    if "åŸ¼ç‰" in (venue or ""): return "åŸ¼ç‰"
    if "æ±äº¬" in (venue or "") or "æ±äº¬éƒ½" in (venue or ""): return "æ±äº¬"
    return None

# ============= ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•´å½¢ =============
def format_event_message(ev):
    name = ev.get("name","").strip()
    start = ev.get("start","") or (ev.get("startDate","")[:10] if ev.get("startDate") else "")
    end = ev.get("end","") or (ev.get("endDate","")[:10] if ev.get("endDate") else "")
    date_line = f"{start} ï½ {end}" if start and end else (start or "")
    venue = ev.get("venue","") or (ev.get("location") or {}).get("name","")
    url = ev.get("official_url") or ev.get("url") or ""
    return f"ğŸª {name}\nğŸ“… {date_line}\nğŸ“ {venue}\nğŸ”— {url}"

# ============= ãƒ¡ã‚¤ãƒ³å‡¦ç† =============
def main():
    logging.info("ğŸš€ START")
    sheet = init_sheet()

    # Walker+ å„éƒ½çœŒã®ä¸€è¦§ (å¾“æ¥é€šã‚Š)
    prefectures = {
        "æ±äº¬": "https://www.walkerplus.com/event_list/ar0313/",
        "ç¥å¥ˆå·": "https://www.walkerplus.com/event_list/ar0314/",
        "åƒè‘‰": "https://www.walkerplus.com/event_list/ar0312/",
        "åŸ¼ç‰": "https://www.walkerplus.com/event_list/ar0311/"
    }

    # prepare buckets
    messages_map = {pref: [f"ğŸ“ {pref} ã®æ–°ç€ã‚¤ãƒ™ãƒ³ãƒˆ ğŸª"] for pref in prefectures.keys()}

    # WALKER+ : per-pref up to 10 new events
    PER_PREF_LIMIT = 10
    for pref, url in prefectures.items():
        events = fetch_walkerplus_events(url, max_pages=2)
        new_count = 0
        for ev in events:
            url_key = ev.get("url") or ""
            if not url_key: 
                continue
            if already_sent(url_key, sheet):
                continue
            msg = format_event_message({
                "name": ev.get("name"),
                "startDate": ev.get("startDate"),
                "endDate": ev.get("endDate"),
                "location": ev.get("location"),
                "official_url": ev.get("url")
            })
            messages_map[pref].append(msg)
            # save using canonical URL as key
            save_event(sheet, {"name": ev.get("name"), "startDate": ev.get("startDate"), "endDate": ev.get("endDate"), "url": url_key, "venue": (ev.get("location") or {}).get("name","")})
            new_count += 1
            if new_count >= PER_PREF_LIMIT:
                break

    # TOKYO ART BEAT : fetch up to 10 events total, map to prefectures and append
    TAB_LIST_URL = "https://www.tokyoartbeat.com/events/condId/most_popular/filter/open"
    tab_events = fetch_tokyoartbeat_officials(TAB_LIST_URL, max_items=10)
    tab_new_count = 0
    for ev in tab_events:
        # decide prefecture
        pref = map_venue_to_prefecture(ev.get("venue",""), ev.get("official_url",""))
        if not pref:
            pref = "æ±äº¬"  # fallback to Tokyo
        # dedupe key: prefer official_url, else name+start
        dedupe_key = ev.get("official_url") or (ev.get("name","") + "_" + (ev.get("start","") or ""))
        if not dedupe_key:
            continue
        if already_sent(dedupe_key, sheet):
            continue
        messages_map.setdefault(pref, [f"ğŸ“ {pref} ã®æ–°ç€ã‚¤ãƒ™ãƒ³ãƒˆ ğŸª"])
        messages_map[pref].append(format_event_message(ev))
        save_event(sheet, {"name": ev.get("name"), "start": ev.get("start"), "end": ev.get("end"), "official_url": ev.get("official_url"), "url": dedupe_key, "venue": ev.get("venue")})
        tab_new_count += 1
        if tab_new_count >= 10:
            break

    # é€ä¿¡: å„éƒ½çœŒã®ãƒã‚±ãƒƒãƒˆã‚’é †ã«é€ä¿¡
    order = ["æ±äº¬","ç¥å¥ˆå·","åƒè‘‰","åŸ¼ç‰"]
    CHUNK_SIZE = 3500
    any_sent = False
    for pref in order:
        bucket = messages_map.get(pref, [])
        if len(bucket) <= 1:
            continue
        # combine bucket into single text message (split if too large)
        combined = "\n\n".join(bucket)
        # split into reasonable chunks for LINE
        parts = [combined[i:i+CHUNK_SIZE] for i in range(0, len(combined), CHUNK_SIZE)]
        for p in parts:
            send_line_broadcast(p)
            time.sleep(1)
        any_sent = True

    if not any_sent:
        logging.info("No new events to send.")
    logging.info("ğŸ END")

if __name__ == "__main__":
    main()
